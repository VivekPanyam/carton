import LanguageSelect, {LanguageSwitch, LanguageItem} from "@/components/languageselect";

<LanguageSwitch>
<LanguageItem forLang='python,rust'>

# Loading a model

<div className="flex items-center space-x-3 not-prose">
    <p>Select a programming language:</p>
    <LanguageSelect/>
</div>

Once you have a packed model, you can pass in a file path or URL to the model.

```python forLang='python'
import asyncio
import cartonml as carton
import numpy as np

async def main():
    # Note this might take a while the first time you use Carton.
    # Make sure to enable logging as described in the quickstart
    model = await carton.load("https://carton.pub/google-research/bert-base-uncased")
    out = await model.infer({
        "input": np.array(["Today is a good [MASK]."])
    })
    print(out)
    # {
    #     'scores': array([[12.977381]]),
    #     'tokens': array([['day']], dtype='<U3')
    # }

asyncio.run(main())
```

```rust forLang='rust'
use carton::Carton;
use carton::types::{LoadOpts, Tensor};

#[tokio::main]
async fn main() {
    // See the logging section in the quickstart guide
    env_logger::Builder::from_env(env_logger::Env::default().default_filter_or("info")).init();

    // Load the model
    let model = Carton::load(
        "https://carton.pub/google-research/bert-base-uncased",
        LoadOpts::default(),
    )
    .await
    .unwrap();

    // Create an ndarray with our inputs
    let arr = ndarray::ArrayD::from_shape_vec(
        ndarray::IxDyn(&[1]),
        vec!["Today is a good [MASK].".to_owned()],
    )
    .unwrap();

    // Run inference
    let out = model
        .infer([("input", Tensor::new(arr))])
        .await
        .unwrap();
    println!("{out:?}");
}
```

<LanguageSwitch>
<LanguageItem forLang='rust'>

To run the above, you'll also need these dependencies in your `Cargo.toml` file:

```toml
[dependencies]
tokio = {version = "1", features = ["macros", "rt-multi-thread"]}
ndarray = "0.15"
env_logger = "0.10"
```

</LanguageItem>
</LanguageSwitch>

Carton loads the model (caching it locally if necessary).

If you need a packed model, take a look at the [packing docs](/docs/packing) or explore the [community model registry](https://carton.pub).

## Load an unpacked model

Carton also supports loading an unpacked model via the `load_unpacked` method. This is conceptually the same as `pack` followed by `load`, but is implemented more efficiently internally. It supports all the options that `load` and `pack` support.

See the [quickstart guide](/quickstart) for an example.

# Options

There are a few options you can pass in when loading a model, but none of them are required.

### `visible_device`
<div className='text-slate-500'>Type: string</div>

The device that is visible to this model.

Allowed values:
 - `cpu`
 - A GPU index (e.g. `0`, `1`, etc.)
 - A GPU UUID (including the `GPU-` or `MIG-GPU-` prefix).<br/>See https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars for more details.

The default is GPU 0 (or CPU if no GPUs are available).

Note: a visible device does not necessarily mean that the model will use that device; it is up to the model to actually use it (e.g. by moving itself to GPU if it sees one available).

Note: If a GPU index is specified, but no GPUs are available, Carton will print a warning and attempt to fallback to CPU

```python forLang='python'
await carton.load(
    # ...
    visible_device = "0",
)
```

```rust forLang='rust'
use carton::Carton;
use carton::types::{Device, LoadOpts};

Carton::load(
    // ...
    LoadOpts {
        // ...
        visible_device: Device::maybe_from_index(0),
        // visible_device: Device::maybe_from_string("0").unwrap(),
        // visible_device: Device::maybe_from_string("cpu").unwrap(),
    },
)
```

### `override_runner_opts`
<div className='text-slate-500'>Type: (see below)</div>

Options to pass to the runner. These are runner-specific (e.g. PyTorch, TensorFlow, etc).

Overrides are merged with the options set when packing the model.

These are sometimes used to configure thread-pool sizes, etc.

For allowed values, see the [packing docs](/docs/packing) for each framework.

```python forLang='python'
await carton.load(
    # ...
    override_runner_opts = {
        # For example, if we know this is a torchscript model and we want to set
        # threading configuration for running this model.
        "num_interop_threads": 4,
        "num_threads": 1,
    },
)
```

```rust forLang='rust'
use carton::Carton;
use carton::types::{Device, LoadOpts, RunnerOpt};

Carton::load(
    // ...
    LoadOpts {
        // ...
        override_runner_opts: Some(
            [
                // For example, if we know this is a torchscript model and we want to set
                // threading configuration for running this model.
                ("num_interop_threads".into(), RunnerOpt::Integer(4)),
                ("num_threads".into(), RunnerOpt::Integer(1)),
            ].into(),
        ),
    },
)
```

### `override_required_framework_version`
<div className='text-slate-500'>Type: string</div>

This is a semver version range that specifies the version of the framework that the model requires.

See https://docs.rs/semver/1.0.16/semver/enum.Op.html and https://docs.rs/semver/1.0.16/semver/struct.VersionReq.html for more details on version ranges.

This is useful if a model is limited to a specific framework version range and you want to override it.

Note: this is not guaranteed to work if the underlying model isn't compatible with the version range you specify.

```python forLang='python'
await carton.load(
    # ...

    # If we know this is a python model and we want to force it to
    # run with a `3.10.x` version of python.
    override_required_framework_version = "=3.10",
)
```

```rust forLang='rust'
use carton::Carton;
use carton::types::{Device, LoadOpts};

Carton::load(
    // ...
    LoadOpts {
        // ...

        // If we know this is a python model and we want to force it to
        // run with a `3.10.x` version of python.
        override_required_framework_version: Some("=3.10".into()),
    },
)
```
</LanguageItem>
<LanguageItem forLang='c'>

# Loading a model

<div className="flex items-center space-x-3 not-prose">
    <p>Select a programming language:</p>
    <LanguageSelect/>
</div>

Once you have a packed model, you can load it with a file path or URL. There are two ways to load a model from C. The first uses callbacks:

```c forLang='c'
#include "carton.h"

void load_callback(Carton *model, CartonStatus status, void *callback_arg);

int main()
{
    int load_callback_arg = 123;
    carton_load("https://carton.pub/google-research/bert-base-uncased", load_callback, (void *)load_callback_arg);

    // Make sure the program doesn't end before the async task completes
    sleep(60);
}
```

This approach can be useful for integrating with existing async/event systems like [libevent](https://libevent.org/) or [libuv](https://libuv.org/).

One of the caveats is that these callbacks are executed from within opaque Carton-owned threads. As a general rule, it's important to avoid doing blocking I/O or extended periods of CPU-bound work within these callbacks. Doing so could block other tasks within Carton's internal async runtime from executing.

To help ease these restrictions, we provide another approach with `CartonAsyncNotifier`. This lets users wait for or poll for new callbacks on a thread they control, removing the above restrictions and making basic C integrations simpler:

```c forLang='c'
#include "carton.h"

int main()
{
    // Create an async notifier
    CartonAsyncNotifier *notifier;
    carton_async_notifier_create(&notifier);

    // Load the model
    Carton *model;
    CartonNotifierCallback callback;
    void *callback_arg = (void *)23;
    carton_async_notifier_register(notifier, &callback, &callback_arg);
    carton_load("https://carton.pub/google-research/bert-base-uncased", (CartonLoadCallback)callback, callback_arg);

    // Wait for the model to load
    void *notifier_callback_arg_out;
    CartonStatus status;
    carton_async_notifier_wait(notifier, (void **)&model, &status, &notifier_callback_arg_out);
    assert(notifier_callback_arg_out == (void *)23);

    // ...
}
```

With both approaches, Carton loads the model (caching it locally if necessary).

Callbacks and notifications are available for every async function in Carton's interface.

For complete, runnable examples of loading and running a model from C, take a look at this [callback example](https://github.com/VivekPanyam/carton/blob/main/source/carton-bindings-c/tests/basic.c) or this [notifier example](https://github.com/VivekPanyam/carton/blob/main/source/carton-bindings-c/tests/notifier.c).

For more info on async functions in the Carton C interface, see [here](https://github.com/VivekPanyam/carton/blob/main/source/carton-bindings-c/README.md).

If you don't yet have a packed model, take a look at the [packing docs](/docs/packing) or explore the [community model registry](https://carton.pub).


</LanguageItem>
<LanguageItem forLang='c++'>

# Loading a model

<div className="flex items-center space-x-3 not-prose">
    <p>Select a programming language:</p>
    <LanguageSelect/>
</div>

Once you have a packed model, you can load it with a file path or URL. There are three ways to load a model from C++. The first uses futures:

```cpp forLang='c++'
#include <iostream>

#include "carton.hh"

int main()
{
    // Load a model, wait for the future to complete, unwrap the result
    auto model = carton::Carton::load("https://carton.pub/google-research/bert-base-uncased").get().get_or_throw();

    // Create an input tensor
    uint64_t shape[]{1};
    auto tensor = carton::Tensor(carton::DataType::kString, shape);
    tensor.at<std::string_view>(0) = "Today is a good [MASK].";

    // Create a map of inputs
    std::unordered_map<std::string, carton::Tensor> inputs;
    inputs.insert(std::make_pair("input", std::move(tensor)));

    // Run inference, wait for the future to complete, unwrap the result
    auto out = model.infer(std::move(inputs)).get().get_or_throw();

    // Get the output tensors
    const auto tokens = out.get_and_remove("tokens");
    const auto scores = out.get_and_remove("scores");

    const auto scores_data = static_cast<const float *>(scores.data());

    std::cout << "Got output token: " << tokens.at<std::string_view>(0) << std::endl;
    std::cout << "Got output scores: " << scores_data[0] << std::endl;
}
```

In the above example, `load` and `infer` both return `std::future`s.

The second approach uses callbacks:

```cpp forLang='c++'
#include "carton.hh"

void load_callback(carton::Result<carton::Carton> model_result, void *arg);

int main()
{
    // Load a model
    int load_callback_arg = 42;
    carton::Carton::load(
        "https://carton.pub/google-research/bert-base-uncased",
        load_callback,
        (void *)load_callback_arg);

    // Make sure the program doesn't end before the async task completes
    sleep(60);
}
```

This approach can be useful for integrating with existing async/event systems like [libevent](https://libevent.org/) or [libuv](https://libuv.org/).

One of the caveats is that these callbacks are executed from within opaque Carton-owned threads. As a general rule, it's important to avoid doing blocking I/O or extended periods of CPU-bound work within these callbacks. Doing so could block other tasks within Carton's internal async runtime from executing.

To help ease these restrictions, we provide a third approach with `carton::AsyncNotifier`. This lets users wait for or poll for new callbacks on a thread they control, removing the above restrictions. It also makes waiting on several futures more efficient because you get notified when one is ready instead of having to poll all of them.

```cpp forLang='c++'
#include "carton.hh"

int main()
{
    // Create an async notifier
    carton::AsyncNotifier<carton::Carton> load_notifier;

    // Load a model
    carton::Carton::load(
        "https://carton.pub/google-research/bert-base-uncased",
        load_notifier.handle(),
        (void *)42);

    // Wait for the model to load
    auto [model_result, arg] = load_notifier.wait();
    assert(arg == (void *)42);
    auto model = model_result.get_or_throw();

    // ...
}
```

With all three approaches, Carton loads the model (caching it locally if necessary).

Futures, callbacks and notifications are available for every async function in Carton's interface.

For complete, runnable examples of loading and running a model from C++, take a look at this [callback example](https://github.com/VivekPanyam/carton/blob/main/source/carton-bindings-cpp/tests/callback.cc), this [future example](https://github.com/VivekPanyam/carton/blob/main/source/carton-bindings-cpp/tests/future.cc) or this [notifier example](https://github.com/VivekPanyam/carton/blob/main/source/carton-bindings-cpp/tests/notifier.cc).

For more info on async functions in the Carton C++ interface, see [here](https://github.com/VivekPanyam/carton/blob/main/source/carton-bindings-cpp/README.md).

If you don't yet have a packed model, take a look at the [packing docs](/docs/packing) or explore the [community model registry](https://carton.pub).


</LanguageItem>
<LanguageItem>

This language currently does not support loading models. Please check the [quickstart guide](/quickstart) for more info or select a different language.

<div className="flex not-prose">
    <LanguageSelect/>
</div>
</LanguageItem>
</LanguageSwitch>


import DocsLayout from '@/components/docslayout'
export default ({children}) => <DocsLayout>{children}</DocsLayout>